{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOECBC/BcGGGXaWli0nNLx5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SaketMunda/tensorflow-fundamentals/blob/master/01_neural_network_regression_exercises.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Neural Network Regression with TensorFlow Exercises\n",
        "\n",
        "In this notebook, we're going to do some exercises related to Regression problem in Neural Network with TensorFlow."
      ],
      "metadata": {
        "id": "spjC9uSm2YXD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import the libraries\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "w0VHlBaO3OLK"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create your own regression dataset and build fit a model to it."
      ],
      "metadata": {
        "id": "5eZLPV1D2pwg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating features\n",
        "X = np.arange(0, 1000, 5)\n",
        "\n",
        "# Creating lables\n",
        "y = np.arange(100, 1100, 5)\n",
        "\n",
        "# The relation between X and y is y = x + 100"
      ],
      "metadata": {
        "id": "kScxv1ln20Ko"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the data\n",
        "plt.scatter(X, y);"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "V4utXOcj3qOC",
        "outputId": "50778d35-450f-4143-ec4d-130c5307b5f0"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X, y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hUEMF4MS3ua8",
        "outputId": "a2c34855-28fc-412a-ab96-319015de89dc"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([  0,   5,  10,  15,  20,  25,  30,  35,  40,  45,  50,  55,  60,\n",
              "         65,  70,  75,  80,  85,  90,  95, 100, 105, 110, 115, 120, 125,\n",
              "        130, 135, 140, 145, 150, 155, 160, 165, 170, 175, 180, 185, 190,\n",
              "        195, 200, 205, 210, 215, 220, 225, 230, 235, 240, 245, 250, 255,\n",
              "        260, 265, 270, 275, 280, 285, 290, 295, 300, 305, 310, 315, 320,\n",
              "        325, 330, 335, 340, 345, 350, 355, 360, 365, 370, 375, 380, 385,\n",
              "        390, 395, 400, 405, 410, 415, 420, 425, 430, 435, 440, 445, 450,\n",
              "        455, 460, 465, 470, 475, 480, 485, 490, 495, 500, 505, 510, 515,\n",
              "        520, 525, 530, 535, 540, 545, 550, 555, 560, 565, 570, 575, 580,\n",
              "        585, 590, 595, 600, 605, 610, 615, 620, 625, 630, 635, 640, 645,\n",
              "        650, 655, 660, 665, 670, 675, 680, 685, 690, 695, 700, 705, 710,\n",
              "        715, 720, 725, 730, 735, 740, 745, 750, 755, 760, 765, 770, 775,\n",
              "        780, 785, 790, 795, 800, 805, 810, 815, 820, 825, 830, 835, 840,\n",
              "        845, 850, 855, 860, 865, 870, 875, 880, 885, 890, 895, 900, 905,\n",
              "        910, 915, 920, 925, 930, 935, 940, 945, 950, 955, 960, 965, 970,\n",
              "        975, 980, 985, 990, 995]),\n",
              " array([ 100,  105,  110,  115,  120,  125,  130,  135,  140,  145,  150,\n",
              "         155,  160,  165,  170,  175,  180,  185,  190,  195,  200,  205,\n",
              "         210,  215,  220,  225,  230,  235,  240,  245,  250,  255,  260,\n",
              "         265,  270,  275,  280,  285,  290,  295,  300,  305,  310,  315,\n",
              "         320,  325,  330,  335,  340,  345,  350,  355,  360,  365,  370,\n",
              "         375,  380,  385,  390,  395,  400,  405,  410,  415,  420,  425,\n",
              "         430,  435,  440,  445,  450,  455,  460,  465,  470,  475,  480,\n",
              "         485,  490,  495,  500,  505,  510,  515,  520,  525,  530,  535,\n",
              "         540,  545,  550,  555,  560,  565,  570,  575,  580,  585,  590,\n",
              "         595,  600,  605,  610,  615,  620,  625,  630,  635,  640,  645,\n",
              "         650,  655,  660,  665,  670,  675,  680,  685,  690,  695,  700,\n",
              "         705,  710,  715,  720,  725,  730,  735,  740,  745,  750,  755,\n",
              "         760,  765,  770,  775,  780,  785,  790,  795,  800,  805,  810,\n",
              "         815,  820,  825,  830,  835,  840,  845,  850,  855,  860,  865,\n",
              "         870,  875,  880,  885,  890,  895,  900,  905,  910,  915,  920,\n",
              "         925,  930,  935,  940,  945,  950,  955,  960,  965,  970,  975,\n",
              "         980,  985,  990,  995, 1000, 1005, 1010, 1015, 1020, 1025, 1030,\n",
              "        1035, 1040, 1045, 1050, 1055, 1060, 1065, 1070, 1075, 1080, 1085,\n",
              "        1090, 1095]))"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into training and test set\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=17)"
      ],
      "metadata": {
        "id": "vg37tjnC3xiB"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's build the model now using Sequential API\n",
        "\n",
        "tf.random.set_seed(17)\n",
        "\n",
        "# Creating a model\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(1)\n",
        "])\n",
        "\n",
        "# compiling a model\n",
        "model.compile(loss=tf.keras.losses.mae,\n",
        "              optimizer=tf.keras.optimizers.Adam(),\n",
        "              metrics=[\"mae\"])\n",
        "\n",
        "# fitting a model\n",
        "model.fit(tf.expand_dims(X_train, axis=-1), y_train, epochs=100, verbose = 0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3-6BDGuJ4Pfq",
        "outputId": "426d4efc-ba67-4f57-960f-332497f147a9"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f5a8e2bcbd0>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# let's evaluate\n",
        "model.evaluate(X_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gfm4xpMm49D_",
        "outputId": "07a69e0e-995f-4e79-b812-eca7b253ec30"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2/2 [==============================] - 0s 9ms/step - loss: 999.2508 - mae: 727.5422\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[999.2507934570312, 727.542236328125]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Try building a neural network with 4 Dense layers and fitting it to your own regression dataset, how does it perform ?"
      ],
      "metadata": {
        "id": "WyBpQHpQ5II-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a model with 4 dense layers\n",
        "\n",
        "tf.random.set_seed(17)\n",
        "\n",
        "# Create a model with 4 dense layers\n",
        "model_1 = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(100),\n",
        "    tf.keras.layers.Dense(10),\n",
        "    tf.keras.layers.Dense(1),\n",
        "    tf.keras.layers.Dense(1)\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model_1.compile(loss=tf.keras.losses.mae,\n",
        "                optimizer=tf.keras.optimizers.Adam(),\n",
        "                metrics=[\"mae\"])\n",
        "\n",
        "# fit the model\n",
        "model_1.fit(tf.expand_dims(X_train, axis = -1), y_train, epochs = 100, verbose = 1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P1I2pVns5Udn",
        "outputId": "53c39fa9-a1e3-418f-869d-ded8553c8ade"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 601.8776 - mae: 601.8776\n",
            "Epoch 2/100\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 478.1723 - mae: 478.1723\n",
            "Epoch 3/100\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 351.9602 - mae: 351.9602\n",
            "Epoch 4/100\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 220.7780 - mae: 220.7780\n",
            "Epoch 5/100\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 86.6110 - mae: 86.6110\n",
            "Epoch 6/100\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 75.0849 - mae: 75.0849\n",
            "Epoch 7/100\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 90.6182 - mae: 90.6182\n",
            "Epoch 8/100\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 61.3493 - mae: 61.3493\n",
            "Epoch 9/100\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 49.6355 - mae: 49.6355\n",
            "Epoch 10/100\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 49.6960 - mae: 49.6960\n",
            "Epoch 11/100\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 45.9104 - mae: 45.9104\n",
            "Epoch 12/100\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 44.6272 - mae: 44.6272\n",
            "Epoch 13/100\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 43.6511 - mae: 43.6511\n",
            "Epoch 14/100\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 43.2610 - mae: 43.2610\n",
            "Epoch 15/100\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 43.9522 - mae: 43.9522\n",
            "Epoch 16/100\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 43.7797 - mae: 43.7797\n",
            "Epoch 17/100\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 43.4615 - mae: 43.4615\n",
            "Epoch 18/100\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 43.0826 - mae: 43.0826\n",
            "Epoch 19/100\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 43.0049 - mae: 43.0049\n",
            "Epoch 20/100\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 43.0032 - mae: 43.0032\n",
            "Epoch 21/100\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 42.8976 - mae: 42.8976\n",
            "Epoch 22/100\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 42.8111 - mae: 42.8111\n",
            "Epoch 23/100\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 42.8633 - mae: 42.8633\n",
            "Epoch 24/100\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 42.7586 - mae: 42.7586\n",
            "Epoch 25/100\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 42.8081 - mae: 42.8081\n",
            "Epoch 26/100\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 42.7335 - mae: 42.7335\n",
            "Epoch 27/100\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 42.6603 - mae: 42.6603\n",
            "Epoch 28/100\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 42.6053 - mae: 42.6053\n",
            "Epoch 29/100\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 42.6605 - mae: 42.6605\n",
            "Epoch 30/100\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 42.6199 - mae: 42.6199\n",
            "Epoch 31/100\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 42.6769 - mae: 42.6769\n",
            "Epoch 32/100\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 42.7333 - mae: 42.7333\n",
            "Epoch 33/100\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 42.4311 - mae: 42.4311\n",
            "Epoch 34/100\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 42.7778 - mae: 42.7778\n",
            "Epoch 35/100\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 42.4133 - mae: 42.4133\n",
            "Epoch 36/100\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 42.9237 - mae: 42.9237\n",
            "Epoch 37/100\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 42.6530 - mae: 42.6530\n",
            "Epoch 38/100\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 42.6001 - mae: 42.6001\n",
            "Epoch 39/100\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 43.1579 - mae: 43.1579\n",
            "Epoch 40/100\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 42.2959 - mae: 42.2959\n",
            "Epoch 41/100\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 43.2701 - mae: 43.2701\n",
            "Epoch 42/100\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 42.5001 - mae: 42.5001\n",
            "Epoch 43/100\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 42.1800 - mae: 42.1800\n",
            "Epoch 44/100\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 42.6552 - mae: 42.6552\n",
            "Epoch 45/100\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 42.0540 - mae: 42.0540\n",
            "Epoch 46/100\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 42.4509 - mae: 42.4509\n",
            "Epoch 47/100\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 42.4384 - mae: 42.4384\n",
            "Epoch 48/100\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 42.2717 - mae: 42.2717\n",
            "Epoch 49/100\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 42.2734 - mae: 42.2734\n",
            "Epoch 50/100\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 42.3086 - mae: 42.3086\n",
            "Epoch 51/100\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 42.1170 - mae: 42.1170\n",
            "Epoch 52/100\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 41.9158 - mae: 41.9158\n",
            "Epoch 53/100\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 42.1261 - mae: 42.1261\n",
            "Epoch 54/100\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 41.9342 - mae: 41.9342\n",
            "Epoch 55/100\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 42.4888 - mae: 42.4888\n",
            "Epoch 56/100\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 41.7954 - mae: 41.7954\n",
            "Epoch 57/100\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 41.8696 - mae: 41.8696\n",
            "Epoch 58/100\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 41.6605 - mae: 41.6605\n",
            "Epoch 59/100\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 41.7993 - mae: 41.7993\n",
            "Epoch 60/100\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 41.8182 - mae: 41.8182\n",
            "Epoch 61/100\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 41.5766 - mae: 41.5766\n",
            "Epoch 62/100\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 41.8963 - mae: 41.8963\n",
            "Epoch 63/100\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 41.6212 - mae: 41.6212\n",
            "Epoch 64/100\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 41.5145 - mae: 41.5145\n",
            "Epoch 65/100\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 41.4886 - mae: 41.4886\n",
            "Epoch 66/100\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 41.5861 - mae: 41.5861\n",
            "Epoch 67/100\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 41.6246 - mae: 41.6246\n",
            "Epoch 68/100\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 41.4627 - mae: 41.4627\n",
            "Epoch 69/100\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 41.3356 - mae: 41.3356\n",
            "Epoch 70/100\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 41.8730 - mae: 41.8730\n",
            "Epoch 71/100\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 41.2803 - mae: 41.2803\n",
            "Epoch 72/100\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 41.3553 - mae: 41.3553\n",
            "Epoch 73/100\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 41.2446 - mae: 41.2446\n",
            "Epoch 74/100\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 41.4985 - mae: 41.4985\n",
            "Epoch 75/100\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 41.5036 - mae: 41.5036\n",
            "Epoch 76/100\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 41.2726 - mae: 41.2726\n",
            "Epoch 77/100\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 41.3602 - mae: 41.3602\n",
            "Epoch 78/100\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 41.1028 - mae: 41.1028\n",
            "Epoch 79/100\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 41.5607 - mae: 41.5607\n",
            "Epoch 80/100\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 40.5367 - mae: 40.5367\n",
            "Epoch 81/100\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 41.5270 - mae: 41.5270\n",
            "Epoch 82/100\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 41.0088 - mae: 41.0088\n",
            "Epoch 83/100\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 40.9415 - mae: 40.9415\n",
            "Epoch 84/100\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 40.7357 - mae: 40.7357\n",
            "Epoch 85/100\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 40.8334 - mae: 40.8334\n",
            "Epoch 86/100\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 40.8495 - mae: 40.8495\n",
            "Epoch 87/100\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 40.7596 - mae: 40.7596\n",
            "Epoch 88/100\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 40.6138 - mae: 40.6138\n",
            "Epoch 89/100\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 41.1141 - mae: 41.1141\n",
            "Epoch 90/100\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 40.5439 - mae: 40.5439\n",
            "Epoch 91/100\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 40.6779 - mae: 40.6779\n",
            "Epoch 92/100\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 40.4837 - mae: 40.4837\n",
            "Epoch 93/100\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 40.5012 - mae: 40.5012\n",
            "Epoch 94/100\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 40.5418 - mae: 40.5418\n",
            "Epoch 95/100\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 40.3906 - mae: 40.3906\n",
            "Epoch 96/100\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 40.7196 - mae: 40.7196\n",
            "Epoch 97/100\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 40.3342 - mae: 40.3342\n",
            "Epoch 98/100\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 40.3419 - mae: 40.3419\n",
            "Epoch 99/100\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 40.6526 - mae: 40.6526\n",
            "Epoch 100/100\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 40.4626 - mae: 40.4626\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f5a8ec0f9d0>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, it performed really well.\n",
        "\n",
        "* `model`'s `mae` was 727\n",
        "* `model_1`'s `mae` is 40"
      ],
      "metadata": {
        "id": "lOrR_ly36CRC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Try and improve the results we got on the insurance dataset, some things you might want to try include:\n",
        "\n",
        "1. Building a larger model(how does one with 4 dense layers go?) âœ…\n",
        "2. Increasing the number of units in each layer.\n",
        "3. Lookup the documentation of Adam and find out what the first parameter is, what happens if you increase it by 10x?\n",
        "4. What happens if you train for longer (say 300 epochs instead of 200)?"
      ],
      "metadata": {
        "id": "_Xi0KHQd6ZLB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Building a larger model\n",
        "model_1.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TgDQeTuF65ix",
        "outputId": "ba69e14e-1279-4fba-ccff-056fdc0667cd"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_2 (Dense)             (32, 100)                 200       \n",
            "                                                                 \n",
            " dense_3 (Dense)             (32, 10)                  1010      \n",
            "                                                                 \n",
            " dense_4 (Dense)             (32, 1)                   11        \n",
            "                                                                 \n",
            " dense_5 (Dense)             (32, 1)                   2         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,223\n",
            "Trainable params: 1,223\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It performed really well."
      ],
      "metadata": {
        "id": "baq1189E7czo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Increasing the number of units in each layer\n",
        "\n",
        "tf.random.set_seed(17)\n",
        "\n",
        "# Creating a model\n",
        "model_2 = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(110),\n",
        "    tf.keras.layers.Dense(50),\n",
        "    tf.keras.layers.Dense(20),\n",
        "    tf.keras.layers.Dense(10)\n",
        "])\n",
        "\n",
        "# compiling a model\n",
        "model_2.compile(loss=tf.keras.losses.mae,\n",
        "              optimizer=tf.keras.optimizers.Adam(),\n",
        "              metrics=[\"mae\"])\n",
        "\n",
        "# fitting a model\n",
        "model_2.fit(tf.expand_dims(X_train, axis=-1), y_train, epochs=100, verbose = 0)\n",
        "\n",
        "# evaluate the model\n",
        "model_2.evaluate(X_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "78EnL67l7h2K",
        "outputId": "6f6fec6c-8a56-42c7-fd99-800d81fdc5e7"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2/2 [==============================] - 0s 7ms/step - loss: 28.1097 - mae: 20.8233\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[28.109716415405273, 20.82330322265625]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Even better !!!"
      ],
      "metadata": {
        "id": "_gxxXCnO73Kh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Tweaking the first parameter of Adam() optimizer function, i.e learning rate.\n",
        "# by default learning_rate is 0.001, if we increase it by 10x then it would be 0.01, let's see how it perform\n",
        "\n",
        "# set seed\n",
        "tf.random.set_seed(17)\n",
        "\n",
        "# Creating a model\n",
        "model_3 = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(110),\n",
        "    tf.keras.layers.Dense(50),\n",
        "    tf.keras.layers.Dense(20),\n",
        "    tf.keras.layers.Dense(10)\n",
        "])\n",
        "\n",
        "# compiling a model\n",
        "model_3.compile(loss=tf.keras.losses.mae,\n",
        "              optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\n",
        "              metrics=[\"mae\"])\n",
        "\n",
        "# fitting a model\n",
        "model_3.fit(tf.expand_dims(X_train, axis=-1), y_train, epochs=100, verbose = 0)\n",
        "\n",
        "# evaluate the model\n",
        "model_3.evaluate(X_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mg0OfQOK8Dj4",
        "outputId": "16439735-7e6e-4902-e824-2ba9bc6f6b14"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2/2 [==============================] - 0s 7ms/step - loss: 28.8930 - mae: 21.1117\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[28.89300537109375, 21.111698150634766]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `mae` quite increased when we tweaked the learning rate."
      ],
      "metadata": {
        "id": "Z47C_mnx8_RI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. If we train little longer\n",
        "\n",
        "# set seed\n",
        "tf.random.set_seed(17)\n",
        "\n",
        "# Creating a model\n",
        "model_4 = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(110),\n",
        "    tf.keras.layers.Dense(50),\n",
        "    tf.keras.layers.Dense(20),\n",
        "    tf.keras.layers.Dense(10)\n",
        "])\n",
        "\n",
        "# compiling a model\n",
        "model_4.compile(loss=tf.keras.losses.mae,\n",
        "              optimizer=tf.keras.optimizers.Adam(),\n",
        "              metrics=[\"mae\"])\n",
        "\n",
        "# fitting a model\n",
        "model_4.fit(tf.expand_dims(X_train, axis=-1), y_train, epochs=300, verbose = 0)\n",
        "\n",
        "# evaluate the model\n",
        "model_4.evaluate(X_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dAUhnR9Q9MCC",
        "outputId": "bd021f74-30b0-4ac8-d0d6-2b8c831b64b4"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2/2 [==============================] - 0s 6ms/step - loss: 8.9510 - mae: 6.5064\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[8.950998306274414, 6.506350994110107]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "WOAAHHHH !!!!!!!!\n",
        "\n",
        "This is the best so far. It means our model learnt quite very well."
      ],
      "metadata": {
        "id": "InMKiuty9Y4i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import the Boston pricing dataset from TensorFlow tf.keras.datasets and model it.\n",
        "\n",
        "Each record in the dataset contains 13 attributes of houses at different locations around the Boston suburbs.\n",
        "\n",
        "Targets are the median values of the houses at a location (in k$)\n",
        "\n",
        "[Boston Pricing dataset](https://www.tensorflow.org/api_docs/python/tf/keras/datasets/boston_housing/load_data)"
      ],
      "metadata": {
        "id": "ftvzGeCD9ikH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import the dataset\n",
        "(boston_X_train, boston_y_train), (boston_X_test, boston_y_test) = tf.keras.datasets.boston_housing.load_data(\n",
        "    path=\"boston_housing.npz\", test_split=0.2, seed=17)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mwrlgUEU-DIB",
        "outputId": "0b92ca98-d848-43b0-9436-075172ecb4cd"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/boston_housing.npz\n",
            "57026/57026 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# view the dataset\n",
        "boston_X_train[1], boston_y_train[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yBpFjgo2-vkj",
        "outputId": "2c80da3c-d969-4764-fc3a-198e9c8b7835"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([1.0960e-02, 5.5000e+01, 2.2500e+00, 0.0000e+00, 3.8900e-01,\n",
              "        6.4530e+00, 3.1900e+01, 7.3073e+00, 1.0000e+00, 3.0000e+02,\n",
              "        1.5300e+01, 3.9472e+02, 8.2300e+00]), 22.0)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "boston_X_test[1], boston_y_test[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2r4oyx1N-0YW",
        "outputId": "8487bcec-2109-41d4-ed12-4476298fec4f"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([1.8159e-01, 0.0000e+00, 7.3800e+00, 0.0000e+00, 4.9300e-01,\n",
              "        6.3760e+00, 5.4300e+01, 4.5404e+00, 5.0000e+00, 2.8700e+02,\n",
              "        1.9600e+01, 3.9690e+02, 6.8700e+00]), 23.1)"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's do the modelling"
      ],
      "metadata": {
        "id": "W_30QunaAwQd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# set random seed\n",
        "tf.random.set_seed(17)\n",
        "\n",
        "# Creating a model\n",
        "boston_model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(100),\n",
        "    tf.keras.layers.Dense(50),\n",
        "    tf.keras.layers.Dense(20),\n",
        "    tf.keras.layers.Dense(10),\n",
        "])\n",
        "\n",
        "# Compiling the model\n",
        "boston_model.compile(loss=tf.keras.losses.mae,\n",
        "                     optimizer=tf.keras.optimizers.Adam(),\n",
        "                     metrics=[\"mae\"])\n",
        "\n",
        "# Fitting the model\n",
        "boston_model.fit(boston_X_train, boston_y_train, epochs=300, verbose=0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wo_rNiPcBIUr",
        "outputId": "a3bf586b-ee1c-49e8-9a8d-78f7e9e36aa0"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f5a8df949d0>"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluating the model\n",
        "boston_model.evaluate(boston_X_test, boston_y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "armHQh0vBxRS",
        "outputId": "0fb6cd9c-947d-44f2-9b4b-4336c50883c7"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 9 calls to <function Model.make_test_function.<locals>.test_function at 0x7f5a8dfcdb90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4/4 [==============================] - 0s 4ms/step - loss: 3.9877 - mae: 3.9877\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[3.9877281188964844, 3.9877281188964844]"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    }
  ]
}